# novel_extractor
# 小说网页内容提取工具

这是一个用于从小说网站提取正文内容并合并为TXT文件的工具。

## 功能特点

1. **网页提取**：
2. **智能内容提取**：
   - 自动识别并提取小说正文内容
   - 排除广告和无关内容
   - 去除HTML标签
   - 智能分段处理
   - 标点符号全角化
   - 删除多余引号和空行
3. **文件合并**：将多个提取的章节合并为单个TXT文件

## 使用方法

1. 运行程序 `novel_extractor.py`
2. 在界面中填写以下信息：
   - **网页地址**：有三种输入模式
     - **范围模式**：输入小说第一章的URL和最后一章的URL（地址格式应相似，只有章节数字不同）
     - **批量模式**：勾选"粘贴批量网页地址"，在文本框中每行输入一个网址
     - **目录模式**：输入小说目录页面URL，自动提取章节链接
   - **输出设置**：设置小说保存路径和文件名
   - **自定义删除规则**：添加需要从文本中删除的内容规则
3. 功能按钮说明：
   - **开始提取**：仅提取内容不应用规则
   - **提取并应用规则**：提取内容并应用自定义规则
   - **应用规则**：对当前设置的输出文件应用自定义规则（不再需要选择文件）
   - **停止**：中止当前正在执行的任务(有BUG不能停止，请直接关闭程序）
   - **清空**：清空所有输入字段
   - **预览**：预览处理后的文件内容
   - 建议先预下载10章左右，打开预览看是否正确下载，以免浪费时间。
   - 批量粘贴链接，可以在CHROME里安装LINK-GRABBER之类的插件来批量获取下载地址。
   - 优先使用范围模式和批量模式，目录模式因网页中存在没有章节名的会优先下载造成混乱。
   - 因为不同的站，在章节开头结一般都有一些有规很的字符，可以在预览里看那些不需要的，运用删除、替换规则进行净化。
4. 合并前的临时文件，默认在合并文件后会删除。

## 系统要求

- Python 3.6 或更高版本
- 依赖库：
  - tkinter
  - requests
  - beautifulsoup4

## 安装依赖

```
pip install requests beautifulsoup4
```

## 注意事项

- 本工具适用于大多数常见的小说网站，但可能需要根据特定网站调整内容提取逻辑
- 处理大量章节时可能需要较长时间，请耐心等待
- 如遇到网络问题，可能需要重试
